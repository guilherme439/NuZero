[running]
state_cache = 'disabled'
running_mode = "sequential"
num_actors = 6
early_fill = 120
early_softmax_exploration = 0.5
early_random_exploration = 0.3

[sequential]
num_games_per_batch = 12
num_batches = 10000

[asynchronous]
training_steps = 1e5
early_delay = 6000
update_delay = 1600

[saving]
storage_frequency = 1
save_frequency = 10

[testing]
early_testing = False
policy_test_frequency = 30
mcts_test_frequency = 60
num_policy_test_games = 100
num_mcts_test_games = 100

[plotting]
plot_frequency = 5
policy_split = 0

[recurrent_networks]
num_train_iterations = 10
num_pred_iterations = 10
num_test_iterations = 10

[learning]
shared_storage_size = 3
replay_window_size = 10000
batch_extraction = 'local'
value_loss = 'SE'
policy_loss = 'CEL'
normalize_cel = False
skip_target = 'policy'
skip_frequency = 0
learning_method = 'samples'

[epochs]
batch_size = 2048
learning_epochs = 1
plot_epoch = False
test_set = False
num_test_set_games = 0

[samples]
batch_size = 1024
num_samples = 3
late_heavy = True

[optimizer]
optimizer = 'Adam'
learning_rate = 1e-5
scheduler_boundaries = [600, 2000, 10000]
scheduler_gamma = 0.5
weight_decay = 1e-7
momentum = 0.9
nesterov = True
