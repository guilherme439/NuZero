[optimization]
state_cache = 'disabled'

[actors]
num_actors = 2
chunk_size = 256

[running]
early_fill = 0
early_softmax_exploration = 0.1
early_random_exploration = 0.1
num_games_per_batch = 2
num_batches = 600
early_testing = False

[saving]
storage_frequency = 1
save_frequency = 1

[testing]
policy_test_frequency = 2
mcts_test_frequency = 10
num_policy_test_games = 100
num_mcts_test_games = 10

[plotting]
plot_frequency = 1
policy_split = 25

[recurrent_networks]
num_train_iterations = 2
num_pred_iterations = 2
num_test_iterations = 2

[learning]
shared_storage_size = 3
replay_window_size = 5000
batch_extraction = 'local'
value_loss = "SE"
policy_loss = "CEL"
normalize_CEL = False
skip_target = 'policy'
skip_frequency = 0
learning_method = 'samples'

[epochs]
batch_size = 1024
learning_epochs = 5
plot_epoch = True
test_set = False
num_test_set_games = 0

[samples]
batch_size = 32
num_samples = 4
late_heavy = False

[optimizer]
optimizer = 'SGD'
learning_rate = 1e-1
scheduler_boundaries = [1800, 10000, 20000]
scheduler_gamma = 0.5
weight_decay = 1e-04
momentum = 0.9