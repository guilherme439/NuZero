[running]
running_mode = 'sequential'
num_actors = 4
early_fill_per_type = 6
early_softmax_moves = 10
early_softmax_exploration = 0.3
early_random_exploration = 0.3
training_steps = 100

[sequential]
num_games_per_type_per_step = 6

[asynchronous]
update_delay = 120

[cache]
cache_choice = 'keyless'
size_estimate = 10000
keep_updated = True

[saving]
storage_frequency = 1
save_frequency = 1

[testing]
asynchronous_testing = True
testing_actors = 1
early_testing = False
policy_test_frequency = 3
mcts_test_frequency = 7
num_policy_test_games = 9
num_mcts_test_games = 9
test_iterations = 6
test_game_index = 0

[plotting]
plot_loss = True
plot_weights = False
plot_frequency = 1
value_split = 0
policy_split = 0
combined_split = 35

[recurrent_training]
train_iterations = [6]
pred_iterations = [6]
alpha = 0.01

[learning]
network_storage_size = 2
replay_window_size = 500
batch_extraction = 'local'
value_loss = 'SE'
policy_loss = 'CEL'
normalize_cel = False
learning_method = 'samples'

[epochs]
batch_size = 2048
learning_epochs = 1
plot_epochs = False

[samples]
batch_size = 16
num_samples = 16
with_replacement = True
late_heavy = False

[optimizer]
optimizer = 'Adam'
learning_rate = 5e-5
scheduler_boundaries = [1000, 50000, 100000]
scheduler_gamma = 0.2
weight_decay = 1e-07
momentum = 0.9
nesterov = True