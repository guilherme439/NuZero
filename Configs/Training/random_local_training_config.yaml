###
Initialization:
  network_name: random_local_small
  load_checkpoint: True

  Checkpoint:
    cp_network_name: random_local_small
    iteration_number: "auto"
    keep_optimizer: True
    keep_scheduler: True
    load_buffer: True
    fresh_start: False
    new_plots: False
    

###
Running:
  running_mode: sequential
  num_actors: 5
  early_fill_per_type: 0
  early_softmax_moves: 0
  early_softmax_exploration: 0.8
  early_random_exploration: 0.5
  training_steps: 5000

  Sequential:
    num_games_per_type_per_step: 10

  Asynchronous:
    update_delay : 120


###
Cache:
  cache_choice: keyless
  max_size: 1000
  keep_updated: True


###
Saving:
  storage_frequency: 1
  save_frequency: 20
  save_buffer: True


###
Testing:
  asynchronous_testing: True
  testing_actors: 2
  early_testing: True
  policy_test_frequency: 25
  mcts_test_frequency: 65
  num_policy_test_games: 100
  num_mcts_test_games: 100
  test_game_index: 0


###
Plotting:
  plot_loss: True
  plot_weights: False
  plot_frequency: 10
  value_split: 0
  policy_split: 0
  combined_split: 0


###
Recurrent Options:
  train_iterations: [6]
  pred_iterations: [6]
  test_iterations: 6
  alpha: 0.01
  

###
Learning:
  shared_storage_size: 2
  replay_window_size: 1500
  batch_extraction: local
  value_loss: SE
  policy_loss: CEL
  normalize_cel: False
  learning_method: samples

  Samples:
    batch_size: 128
    num_samples: 16
    with_replacement: True
    late_heavy: True

  Epochs:
    batch_size: 2048
    learning_epochs: 1
    plot_epochs: False


###
Optimizer:
  optimizer_choice: Adam

  SGD:
    weight_decay: 1.0e-07
    momentum: 0.9
    nesterov: True


###
Scheduler:
  starting_lr: 2.0e-4
  scheduler_boundaries: [24000, 48000, 72000, 96000]
  scheduler_gamma: 0.5